---
title: Overview
sidebar_position: 3
---

TODO

Participants wore Aria glasses to record the egocentirc perspective, enabling
sub-millisecond accurate IMU data, calibrated cameras (intrinsic parameters)
with 3D trajectories (camera pose/extrinsic parameters) of the camera wearer &
3D point clouds of the recorded environment. Lastly, a web-based visualization
tool to browse the dataset is available.

There are seven benchmark tasks with associated annotations. At a high-level,
these tasks aim to understand procedural activities/tasks (TODO: fixme; "Keystep") &
relate objects between multiple viewpoints/cameras ("Relations"). Not directly
related to the benchmark tasks, we annotated & collected natural language data
aligned to the video data. There are three forms of natural langauge data: (1) what
was happening in the video (atomic action descriptions), (2) commentaries on the
performance of the tasks (expert commentary), and (3) the participants
self-narrating their actions as they perform the task/activity (narrate & act). 

## Summary of Dataset

What do want to summarize here?
- 

| abc | def
|:---|:---

## How the Data Was Recorded
TODO

## Benchmarks
TODO

