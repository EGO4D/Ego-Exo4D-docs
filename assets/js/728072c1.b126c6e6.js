"use strict";(self.webpackChunkegoexo_docs=self.webpackChunkegoexo_docs||[]).push([[7948],{3905:(e,a,t)=>{t.d(a,{Zo:()=>l,kt:()=>g});var o=t(7294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);a&&(o=o.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,o)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,o,n=function(e,a){if(null==e)return{};var t,o,n={},r=Object.keys(e);for(o=0;o<r.length;o++)t=r[o],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)t=r[o],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var p=o.createContext({}),c=function(e){var a=o.useContext(p),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},l=function(e){var a=c(e.components);return o.createElement(p.Provider,{value:a},e.children)},d="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return o.createElement(o.Fragment,{},a)}},_=o.forwardRef((function(e,a){var t=e.components,n=e.mdxType,r=e.originalType,p=e.parentName,l=s(e,["components","mdxType","originalType","parentName"]),d=c(t),_=n,g=d["".concat(p,".").concat(_)]||d[_]||m[_]||r;return t?o.createElement(g,i(i({ref:a},l),{},{components:t})):o.createElement(g,i({ref:a},l))}));function g(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var r=t.length,i=new Array(r);i[0]=_;var s={};for(var p in a)hasOwnProperty.call(a,p)&&(s[p]=a[p]);s.originalType=e,s[d]="string"==typeof e?e:n,i[1]=s;for(var c=2;c<r;c++)i[c]=t[c];return o.createElement.apply(null,i)}return o.createElement.apply(null,t)}_.displayName="MDXCreateElement"},3046:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>p,contentTitle:()=>i,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});var o=t(7462),n=(t(7294),t(3905));const r={},i="Tutorial 2: Hand and Body Pose in Ego-Exo4D Dataset",s={unversionedId:"tutorials/pose",id:"tutorials/pose",title:"Tutorial 2: Hand and Body Pose in Ego-Exo4D Dataset",description:"Status",source:"@site/docs/tutorials/pose.md",sourceDirName:"tutorials",slug:"/tutorials/pose",permalink:"/tutorials/pose",draft:!1,tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Tutorial 1: Gaze in Ego-Exo4D Dataset",permalink:"/tutorials/gaze"},next:{title:"Tutorial 3: Undistort frames and overlay annotations",permalink:"/tutorials/undistort"}},p={},c=[{value:"1. Prerequisites and Imports",id:"1-prerequisites-and-imports",level:3},{value:"2. Load one sample take and its ego / exo camera&#39;s camera calibration.",id:"2-load-one-sample-take-and-its-ego--exo-cameras-camera-calibration",level:3},{value:"2.1 load exocentric camera calibrations",id:"21-load-exocentric-camera-calibrations",level:4},{value:"2.2 load egocentric camera calibrations",id:"22-load-egocentric-camera-calibrations",level:4},{value:"3. Load body / hand pose and project it to exocentric views",id:"3-load-body--hand-pose-and-project-it-to-exocentric-views",level:3},{value:"3. Projecting pose to egocentric view using camera info",id:"3-projecting-pose-to-egocentric-view-using-camera-info",level:3},{value:"Conclusion",id:"conclusion",level:3}],l={toc:c},d="wrapper";function m(e){let{components:a,...r}=e;return(0,n.kt)(d,(0,o.Z)({},l,r,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"tutorial-2-hand-and-body-pose-in-ego-exo4d-dataset"},"Tutorial 2: Hand and Body Pose in Ego-Exo4D Dataset"),(0,n.kt)("p",null,(0,n.kt)("img",{parentName:"p",src:"https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=green",alt:"Status"})),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Filled notebook:"),"\n",(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/Ego4d/tree/main/notebooks/egoexo"},(0,n.kt)("img",{parentName:"a",src:"https://img.shields.io/static/v1.svg?logo=github&label=Tutorial&message=View%20On%20Github&color=lightgrey",alt:"View on Github"})),"\n",(0,n.kt)("strong",{parentName:"p"},"Author:")," Xizi Wang"),(0,n.kt)("p",null,"3D hand and body pose are two important annotations of the Ego-Exo4D dataset. The figure on the left captures the ",(0,n.kt)("strong",{parentName:"p"},"full body pose")," and surrounding environment context, whereas the figure on the right captures the details of close-by ",(0,n.kt)("strong",{parentName:"p"},"hand-object")," interactions and the camera wearer\u2019s attention. In this tutorial, we provide a step-by-step guide on retrieving the hand and body pose of one example take, and projecting the body pose to exocentric views and the hand pose to egocentric views, then visualizing it on the corresponding frame."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"Samples of hand and body pose",src:t(2148).Z,width:"1706",height:"778"})),(0,n.kt)("h3",{id:"1-prerequisites-and-imports"},"1. Prerequisites and Imports"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'%matplotlib inline\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nfrom matplotlib import rcParams\nfrom projectaria_tools.core import mps\nfrom projectaria_tools.core import data_provider\nfrom projectaria_tools.core import calibration\nfrom projectaria_tools.core.calibration import CameraCalibration, KANNALA_BRANDT_K3\nfrom projectaria_tools.core.stream_id import StreamId\n\nrcParams["figure.figsize"] = 16, 32\n\nimport json\nimport os\nimport random\n\nimport av\nimport pandas as pd\nfrom PIL import Image, ImageDraw\nfrom tqdm.auto import tqdm\n\n# functions for loading the hand & body pose:\nfrom pose_utilities import get_body_metadata, get_hands_metadata, get_coords\n# functions for loading camera calibrations\nfrom pose_utilities import load_csv_to_df\n# functions for visualization\nfrom pose_utilities import palette,  draw_skeleton, draw_cross, draw_circle, draw_skeleton_hands, draw_circle_hands, draw_cross_hands, show_results, get_viz, get_frame\n')),(0,n.kt)("h3",{id:"2-load-one-sample-take-and-its-ego--exo-cameras-camera-calibration"},"2. Load one sample take and its ego / exo camera's camera calibration."),(0,n.kt)("p",null,"First, let's define the Ego-Exo4D dataset and its annotation location."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'release_dir = "/datasets01/egoexo4d/v2/" # replace this with your download folder\nannotation_dir = os.path.join(release_dir, "annotations/")  # annotation folder\n\n# load necessary annotation files\negoexo = {\n    "takes": os.path.join(release_dir, "takes.json"),\n    "captures": os.path.join(release_dir, "captures.json")\n}\n\nfor k, v in egoexo.items():\n    egoexo[k] = json.load(open(v))\n\ntakes = egoexo["takes"]\ncaptures = egoexo["captures"]\ntakes_by_uid = {x["take_uid"]: x for x in takes}\n')),(0,n.kt)("p",null,"We then randomly sample one example take of playing piano. We also provide the take uid of some other takes as examples."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'#take_uid = "0bc47e29-e086-4726-b874-f89671366f06"  # Violin\ntake_uid = "23ff1c48-01ea-4d34-a38b-bc96e767b9b9" #Piano\n#take_uid = "02715c86-e30c-4791-92b7-38b488e51aba"  # Bike\n\ntake = [take for take in egoexo["takes"] if take["take_uid"] == take_uid]\ntake = take[0]\n')),(0,n.kt)("p",null,"And load the camera intrinsics and extrinsics of the take. ",(0,n.kt)("strong",{parentName:"p"},"exo_traj_df")," reads in the exocentric cameras calibrations in csv format."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'# Initialize exo cameras from calibration file\ntraj_dir = os.path.join(release_dir, take["root_dir"], "trajectory")\nexo_traj_path = os.path.join(traj_dir, "gopro_calibs.csv")\n\nexo_traj_df = load_csv_to_df(exo_traj_path)\nexo_cam_names = list(exo_traj_df["cam_uid"])\nego_cam_names = [x["cam_id"] for x in take["capture"]["cameras"] if x["is_ego"] and x["cam_id"].startswith("aria")]\nall_cams = ego_cam_names + exo_cam_names\nego_cam_name = ego_cam_names[0]\nprint("exo cameras:\\t", exo_cam_names)\nprint("ego camera:\\t", ego_cam_name)\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"exo cameras:     ['gp01', 'gp02', 'gp03', 'gp04', 'gp05']\nego camera:  aria01\n")),(0,n.kt)("h4",{id:"21-load-exocentric-camera-calibrations"},"2.1 load exocentric camera calibrations"),(0,n.kt)("p",null,"The exocentric camera calibrations can be read using Project Aria Machine Perception Services (MPS). We store them with camera uid as keys in ",(0,n.kt)("strong",{parentName:"p"},"go_pro_proxy"),"."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'go_pro_proxy = {}\nstatic_calibrations = mps.read_static_camera_calibrations(exo_traj_path)\nfor static_calibration in static_calibrations:\n    # assert the GoPro was correctly localized\n    if static_calibration.quality != 1.0:\n        print(f"Camera: {static_calibration.camera_uid} was not localized, ignoring this camera.")\n        continue\n    proxy = {}\n    proxy["name"] = static_calibration.camera_uid\n    proxy["pose"] = static_calibration.transform_world_cam\n    proxy["camera"] = CameraCalibration(\n                            static_calibration.camera_uid,\n                            KANNALA_BRANDT_K3,\n                            static_calibration.intrinsics,\n                            static_calibration.transform_world_cam, # probably extrinsics\n                            static_calibration.width,\n                            static_calibration.height,\n                            None,\n                            math.pi,\n                            "")\n\n    go_pro_proxy[static_calibration.camera_uid] = proxy\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Camera: gp05 was not localized, ignoring this camera.\nLoaded #StaticCameraCalibration data: 5\n")),(0,n.kt)("h4",{id:"22-load-egocentric-camera-calibrations"},"2.2 load egocentric camera calibrations"),(0,n.kt)("p",null,"We read the egocentric camera intrinsics using VRS and camera extrinsics using MPS."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'## Configure the VRSDataProvider (interface used to retrieve Trajectory data)\nego_exo_project_path = os.path.join(release_dir, \'takes\', take[\'take_name\'])\n\naria_dir = os.path.join(release_dir, take["root_dir"])\naria_path = os.path.join(aria_dir, f"{ego_cam_name}.vrs")\nvrs_data_provider = data_provider.create_vrs_data_provider(aria_path)\ndevice_calibration = vrs_data_provider.get_device_calibration()\n\nrgb_stream_id = StreamId("214-1")\nrgb_stream_label = vrs_data_provider.get_label_from_stream_id(rgb_stream_id)\nrgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n\nmps_data_paths_provider = mps.MpsDataPathsProvider(ego_exo_project_path)\nmps_data_paths = mps_data_paths_provider.get_data_paths()\nmps_data_provider = mps.MpsDataProvider(mps_data_paths)\n')),(0,n.kt)("h3",{id:"3-load-body--hand-pose-and-project-it-to-exocentric-views"},"3. Load body / hand pose and project it to exocentric views"),(0,n.kt)("p",null,"In this section, we go through the steps of projecting body / hand pose to exocentric views. We first load the annotation file of the pose. The annotation is a dictionary with the frame indices as keys. As an example, we randomly sample the 3D and 2D annotation of one frame. You can switch annotation_type to choose between body and hand."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'annotation_type = "hand" # annotation_type should be body or hand.\n\n# get body pose annotation folder\negopose_ann_dir = os.path.join(\n    annotation_dir, f"ego_pose/train/{annotation_type}/annotation"\n)\n# get the annotation file of the sampled take\nannotation_file_path = os.path.join(egopose_ann_dir, f"{take_uid}.json")\nall_annotations = json.load(open(annotation_file_path))\n# annotation is a dictionary with frame numbers as keys, we then randomly sample one frame.\nframe_idx = random.sample(list(all_annotations.keys()), 1)[0]\nannotation = all_annotations[frame_idx][0]\nframe_idx = int(frame_idx)\nprint(f"annotation at sampled frame {frame_idx} is {annotation.keys()}.")\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"annotation at sampled frame 3382 is dict_keys(['annotation3D', 'annotation2D']).\n")),(0,n.kt)("p",null,"Next we read the corresponding at the sampled frame index from exocentric videos and egocentric video. We store it in a dictionary ",(0,n.kt)("strong",{parentName:"p"},"videos")," with camera name as key."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"base_directory = os.path.join(release_dir, take[\"root_dir\"])\nvideos = {}\nfor cam_name in all_cams:\n    if cam_name in exo_cam_names:\n        stream_name = '0'\n    else:\n        stream_name = 'rgb'\n\n    local_path = os.path.join(base_directory, take['frame_aligned_videos'][cam_name][stream_name]['relative_path'])\n    container = av.open(local_path)\n    videos[cam_name] = get_frame(local_path, frame_idx)\n")),(0,n.kt)("p",null,"We retrieve the pose in the format of 3D keypoints from the annotation file. 3D keypoints are world coordinates of the pose. Note that annotation also have ",(0,n.kt)("strong",{parentName:"p"},"annotation2D")," which are 2D keypoints annotated on undistorted frames. We will show this part later in projecting hand pose to egocentric frame."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"parts = list(annotation[\"annotation3D\"].keys())\nannot_3d = {}\nfor part in parts:\n    keypoint = annotation[\"annotation3D\"][part]\n    annot_3d[part] = [keypoint['x'], keypoint['y'], keypoint['z']]\n")),(0,n.kt)("p",null,"With exocentric camera calibration, we project the 3D body/hand keypoints to different exocentric views. The process is similar to the reprojection in the gaze tutorial. The body/hand keypoint is first projected from world coordinates to exocentric camera device, then to the exocentric camera image plane."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"per_go_pro_reprojection = {}\nfor go_pro in go_pro_proxy:\n    if go_pro not in per_go_pro_reprojection.keys():\n        per_go_pro_reprojection[go_pro] = {}\n    for part in parts:\n        pose_vector_in_world = annot_3d[part]\n        # project the keypoint from world to go_pro device\n        pose_in_go_pro_world = go_pro_proxy[go_pro][\"pose\"].inverse() @ pose_vector_in_world\n\n        # project the keypoint from go_pro device to go_pro image plane\n        device_projection = go_pro_proxy[go_pro][\"camera\"].project(pose_in_go_pro_world)\n        if device_projection is None:\n            continue\n        else:\n            per_go_pro_reprojection[go_pro][part] = {'x': device_projection[0], 'y': device_projection[1], 'placement': 'manual'}\n")),(0,n.kt)("p",null,"We define the keypoints_map, skeleton and pose_kpt_color for visualization."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'if annotation_type == "body":\n    keypoints_map, skeleton, pose_kpt_color = get_body_metadata()\nelse:\n    keypoints_map, skeleton, pose_kpt_color = get_hands_metadata()\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"projection_results = {}\nfor cam_name in per_go_pro_reprojection.keys():\n    viz_img = get_viz(\n        videos[cam_name],\n        keypoints_map,\n        per_go_pro_reprojection[cam_name],\n        skeleton,\n        pose_kpt_color,\n        annot_type=annotation_type,\n    )\n    projection_results[cam_name] = viz_img\nshow_results(projection_results)\n")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"png",src:t(2892).Z,width:"2757",height:"1590"})),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"png",src:t(9386).Z,width:"2757",height:"1590"})),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"png",src:t(642).Z,width:"2757",height:"1590"})),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"png",src:t(9253).Z,width:"2757",height:"1590"})),(0,n.kt)("h3",{id:"3-projecting-pose-to-egocentric-view-using-camera-info"},"3. Projecting pose to egocentric view using camera info"),(0,n.kt)("p",null,"In this section, we further provide instructions projecting hand pose to egocentric view, since hand-object interaction is an important part of egocentric video analysis. The projection includes the following steps:"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},"retrieve 3D pose of the hand keypoint as world coordinates from annotation"),(0,n.kt)("li",{parentName:"ol"},"retrieve camera intrinsics and extrinsics from camera_info"),(0,n.kt)("li",{parentName:"ol"},"project the 3D keypoint from world to aria device with camera extrinsics"),(0,n.kt)("li",{parentName:"ol"},"project the 3D keypoint from aria device to aria image plane with camera intrinsics"),(0,n.kt)("li",{parentName:"ol"},"get the resulted 2D keypoint")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},'takes_info = json.load(open(os.path.join(release_dir, "takes.json")))\n\n# go through all takes to locate the sampled take, i.e., cmu_bike01_5\ntake_name = take["take_name"]\nimport re\n\n# get the capture name, and load the timesync.csv\ncapture_name = re.sub(r"_\\d+$", "", take_name)\ntimesync = pd.read_csv(os.path.join(release_dir, f"captures/{capture_name}/timesync.csv"))\n\nstart_idx = take["timesync_start_idx"]+1\nend_idx = take["timesync_end_idx"]\ntake_timestamps = []\nfor idx in range(start_idx, end_idx):\n    take_timestamps.append(int(timesync.iloc[idx][f"aria01_214-1_capture_timestamp_ns"]))\nsample_timestamp = take_timestamps[int(frame_idx)]\n')),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"ego_reprojection = {}\npose_info = mps_data_provider.get_closed_loop_pose(sample_timestamp)\n\ncam = ego_cam_name\nfor part in parts:\n    pose_vector_in_world = annot_3d[part]\n\n    if pose_info:\n        # transform coordinates from device to world\n        T_world_device = pose_info.transform_world_device\n\n    T_device_camera = rgb_camera_calibration.get_transform_device_camera()\n    T_world_camera = T_world_device @ T_device_camera\n\n    pose_in_aria_world = T_world_camera.inverse() @ pose_vector_in_world\n    device_projection = rgb_camera_calibration.project(pose_in_aria_world)\n\n    if device_projection is None:\n        continue\n    else:\n        x_coord = device_projection[0]\n        y_coord = device_projection[1]\n        ego_reprojection[part] = {'x': x_coord, 'y': y_coord, 'placement': 'auto'}\n")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Loaded #closed loop trajectory poses records: 646865\n")),(0,n.kt)("p",null,"Now let's visualize the projected hand pose on the aria frame."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-python"},"ego_local_path = os.path.join(base_directory, take['frame_aligned_videos'][ego_cam_name]['rgb']['relative_path'])\nego_frame = get_frame(ego_local_path, frame_idx)\nif annotation_type == 'hand':\n    cam_name = ego_cam_name\n    ann = ego_reprojection\n    img = ego_frame\n    img = img.rotate(90)\n    image_array = np.asarray(img)\n    image = Image.fromarray(image_array, \"RGB\")\n\n    viz_img = get_viz(image, keypoints_map, ann, skeleton, pose_kpt_color, annot_type=annotation_type, is_aria=True)\n    plt.figure(figsize=(8, 8))\n    plt.imshow(viz_img.rotate(270))\n    plt.axis(\"off\")  # Hide the axes ticks\n    plt.title(f\"{cam_name}\")\n    plt.show()\n")),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"png",src:t(7184).Z,width:"636",height:"658"})),(0,n.kt)("h3",{id:"conclusion"},"Conclusion"),(0,n.kt)("p",null,"In this notebook, we reviewed loading takes, its egocentric and exocentric camera calibrations and EgoPose annotations. We also provided a step-by-step guide on projecting 3D hand/body keypoints to different views with the camera calibrations. This can serve as a good starting point to understand EgoPose and utilize it for research in the future."),(0,n.kt)("hr",null),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/Ego4d"},(0,n.kt)("img",{parentName:"a",src:"https://img.shields.io/static/v1.svg?logo=star&label=%E2%AD%90&message=Star%20Our%20Repository&color=yellow",alt:"Star our repository"})),"  If you found this tutorial helpful, consider \u2b50-ing our repository.\n",(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/Ego4d/issues"},(0,n.kt)("img",{parentName:"a",src:"https://img.shields.io/static/v1.svg?logo=star&label=%E2%9D%94&message=Ask%20Questions&color=9cf",alt:"Ask questions"})),"  For any questions, typos, or bugs that you found, please raise an issue on GitHub."),(0,n.kt)("hr",null))}m.isMDXComponent=!0},2148:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/hand_body_pose-1040ecb7222b8a5865e444aa4e5c9c36.png"},2892:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/pose_tutorial-0-018f5305b6365157a797ad2666191c65.png"},9386:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/pose_tutorial-1-23f48afa2431fdddb60f1bdb0741f7d4.png"},642:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/pose_tutorial-2-efa4f4af15232cf9b5e963290d59b612.png"},9253:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/pose_tutorial-3-103cd05fabff53353a72bc3a58047fd5.png"},7184:(e,a,t)=>{t.d(a,{Z:()=>o});const o=t.p+"assets/images/pose_tutorial-4-ddd89629042507c68d8f282cf3d49978.png"}}]);