"use strict";(self.webpackChunkegoexo_docs=self.webpackChunkegoexo_docs||[]).push([[80],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>k});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function i(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var s=n.createContext({}),p=function(e){var t=n.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):i(i({},t),e)),a},m=function(e){var t=p(e.components);return n.createElement(s.Provider,{value:t},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),c=p(a),d=r,k=c["".concat(s,".").concat(d)]||c[d]||u[d]||o;return a?n.createElement(k,i(i({ref:t},m),{},{components:a})):n.createElement(k,i({ref:t},m))}));function k(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=a.length,i=new Array(o);i[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[c]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=a[p];return n.createElement.apply(null,i)}return n.createElement.apply(null,a)}d.displayName="MDXCreateElement"},1016:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>i,default:()=>u,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var n=a(7462),r=(a(7294),a(3905));const o={title:"Change Log",sidebar_position:99},i=void 0,l={unversionedId:"changelog",id:"changelog",title:"Change Log",description:"V2",source:"@site/docs/changelog.md",sourceDirName:".",slug:"/changelog",permalink:"/changelog",draft:!1,tags:[],version:"current",sidebarPosition:99,frontMatter:{title:"Change Log",sidebar_position:99},sidebar:"tutorialSidebar",previous:{title:"EgoExo4D Challenge 2024",permalink:"/challenge"},next:{title:"Contact Us",permalink:"/contact"}},s={},p=[{value:"V2",id:"v2",level:2},{value:"March 28, 2024",id:"march-28-2024",level:3},{value:"March 25, 2024",id:"march-25-2024",level:3},{value:"Data",id:"data",level:4},{value:"Annotations",id:"annotations",level:4},{value:"Accessibility",id:"accessibility",level:4},{value:"V1",id:"v1",level:2},{value:"March 15, 2024",id:"march-15-2024",level:3},{value:"Jan 27, 2024",id:"jan-27-2024",level:3},{value:"Dec 15, 2023",id:"dec-15-2023",level:3}],m={toc:p},c="wrapper";function u(e){let{components:t,...a}=e;return(0,r.kt)(c,(0,n.Z)({},m,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"v2"},"V2"),(0,r.kt)("h3",{id:"march-28-2024"},"March 28, 2024"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"ego4d pip package ",(0,r.kt)("inlineCode",{parentName:"li"},"1.7.1")," released, which starting your download significantly faster than previous versions. Upgrade your pip package!")),(0,r.kt)("h3",{id:"march-25-2024"},"March 25, 2024"),(0,r.kt)("p",null,"Ego-Exo4D V2 is announced. Please see the forum for the announcement post."),(0,r.kt)("p",null,"You may download the new data by upgrading your CLI tool or using the flag ",(0,r.kt)("inlineCode",{parentName:"p"},"--release v2"),"."),(0,r.kt)("p",null,"A summary of changes are listed below:"),(0,r.kt)("h4",{id:"data"},"Data"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"1286.30 total video hours")," across 5035 takes (",(0,r.kt)("strong",{parentName:"li"},"221.26 ego-centric hours"),"). ",(0,r.kt)("em",{parentName:"li"},"Comparing to V1:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"1341 additional takes: spanning 254.8 more total hours (44.4 ego-hours)"),(0,r.kt)("li",{parentName:"ul"},"119 additional Narrate & Act takes (456 takes)"))),(0,r.kt)("li",{parentName:"ul"},"99% of takes contain eye gaze (2D and 3D), trajectory data and 3D point clouds"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Note on Dataset Quality:")," each take is verified to have the correct task ID label by external annotators and verified by university data POCs. Please see this ",(0,r.kt)("a",{parentName:"li",href:"https://discuss.ego4d-data.org/t/ego-exo4d-dataset-changes-quality-issues-future-update/463"},"forum post")," for additional context."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"NEW:"))," Best exocentric labels available for 90% of the takes",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Available in ",(0,r.kt)("a",{parentName:"li",href:"/data/metadata/#takes"},(0,r.kt)("inlineCode",{parentName:"a"},"takes.json"))," under the field ",(0,r.kt)("inlineCode",{parentName:"li"},'"best_exo"')))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"NEW:"))," Tight take time boundaries indicating when the task occurs within a take",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Available in ",(0,r.kt)("a",{parentName:"li",href:"/data/metadata/#takes"},(0,r.kt)("inlineCode",{parentName:"a"},"takes.json"))," under the field ",(0,r.kt)("inlineCode",{parentName:"li"},'"task_start_sec"')," / ",(0,r.kt)("inlineCode",{parentName:"li"},'"task_end_sec"'))))),(0,r.kt)("h4",{id:"annotations"},"Annotations"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," ",(0,r.kt)("a",{parentName:"li",href:"/annotations/expert_commentary"},"Expert Commentary"),": 11,689 annotations covering 3,055 takes (117,812 audio recordings) annotated from 50 experts"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," EgoPose (Body) human generated ground truth available for 1,358 takes containing 376K 3D body poses and 2M 2D body pose annotations. Automatic annotations covering 2559 takes, 9.2M 3D body poses and 46.87M 2D body poses are also available."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," EgoPose (Hand) human generated ground truth available for 458 takes containing 68K 3D hand poses and 340K 2D hand pose annotations. Automatic annotations covering 976 takes, 4.3M 3D hand poses and 21M 2D hand poses are also available."),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," Atomic Actions Descriptions covering 4,965 takes with 432,467 text descriptions. Comparing to V1:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"172% more descriptions (250,742 descriptions in V1) across 2,202 more takes"),(0,r.kt)("li",{parentName:"ul"},'Bug fix: "unsure" and "best exo" fields corrected'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," Relation Annotations 2.2M segmentation masks across 1653 takes. Comparing to V1:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"832,733 more segmentation masks across 396 more takes"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," Train/val/test sets now cover all takes in the dataset: 3072 train/842 val/1121 test"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," Atomic Actions Descriptions covering 4965 takes with 432,467 text descriptions. Comparing to V1:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"172% more descriptions (250,742 descriptions in V1) across 2,202 more takes"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"Bug fix:"),' "unsure" and "best exo" fields corrected'))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," Relation Annotations 2.2M segmentation masks across 1653 takes. Comparing to V1:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"832,733 more segmentation masks across 396 more takes"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"UPDATED:"))," Train/val/test sets now cover all takes in the dataset: 3072 train/842 val/1121 test")),(0,r.kt)("h4",{id:"accessibility"},"Accessibility"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"NEW:"))," Transcriptions and pre-extracted audio all takes, including Narrate & Act takes, where camera-wearers verbally describe what they are doing while they are doing it. ",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Download with ",(0,r.kt)("inlineCode",{parentName:"li"},"--parts take_audio")," and ",(0,r.kt)("inlineCode",{parentName:"li"},"--parts take_transcription")),(0,r.kt)("li",{parentName:"ul"},"Note, this data is only for the egocentric aria file."),(0,r.kt)("li",{parentName:"ul"},"Thanks ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/Ego4d/issues/288"},"@juliawilkins")," for the pre-extracted audio suggestion."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"NEW:"))," ",(0,r.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/Ego4d/pull/301"},"MAWS CLIP")," features for each frame of video in Ego-Exo4D"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},(0,r.kt)("em",{parentName:"strong"},"NEW:"))," 2D eye gaze for 99% of takes (derived from the 3D eye gaze; projected into the egocentric image plane)")),(0,r.kt)("h2",{id:"v1"},"V1"),(0,r.kt)("h3",{id:"march-15-2024"},"March 15, 2024"),(0,r.kt)("p",null,"The following annotations were updated or newly introduced:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"NEW"),": EgoPose (hand & body)"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"NEW"),": Expert Commentary"),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"UPDATED:")," Atomic Action Descriptions")),(0,r.kt)("h3",{id:"jan-27-2024"},"Jan 27, 2024"),(0,r.kt)("p",null,"Quality issues were spotted, with our intention to fix and improve the dataset. See the context on the forums here: ",(0,r.kt)("a",{parentName:"p",href:"https://discuss.ego4d-data.org/t/ego-exo4d-dataset-changes-quality-issues-future-update/463"},"https://discuss.ego4d-data.org/t/ego-exo4d-dataset-changes-quality-issues-future-update/463")),(0,r.kt)("h3",{id:"dec-15-2023"},"Dec 15, 2023"),(0,r.kt)("p",null,"Release of Ego-Exo4D is announced. Please refer to the forum post: ",(0,r.kt)("a",{parentName:"p",href:"https://discuss.ego4d-data.org/t/ego-exo4d-ego4d-v2-1-release/417"},"https://discuss.ego4d-data.org/t/ego-exo4d-ego4d-v2-1-release/417")))}u.isMDXComponent=!0}}]);