"use strict";(self.webpackChunkegoexo_docs=self.webpackChunkegoexo_docs||[]).push([[372],{3905:(e,t,a)=>{a.d(t,{Zo:()=>p,kt:()=>k});var r=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,r)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,r,n=function(e,t){if(null==e)return{};var a,r,n={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=r.createContext({}),c=function(e){var t=r.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},p=function(e){var t=c(e.components);return r.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},d=r.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(a),d=n,k=u["".concat(s,".").concat(d)]||u[d]||m[d]||i;return a?r.createElement(k,o(o({ref:t},p),{},{components:a})):r.createElement(k,o({ref:t},p))}));function k(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,o=new Array(i);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:n,o[1]=l;for(var c=2;c<i;c++)o[c]=a[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}d.displayName="MDXCreateElement"},6777:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var r=a(7462),n=(a(7294),a(3905));const i={title:"Overview",sidebar_position:3},o=void 0,l={unversionedId:"overview",id:"overview",title:"Overview",description:"Ego-Exo4D is a large-scale multi-modal multi-view video dataset (including 3D) and benchmark challenge. The dataset consists of time-synchronized videos of participants recorded with at least one first-person (egocentric Aria glasses) and third-person (exocentric GoPro cameras) perspective cameras. Recordings occurred around the world from 12 different research institutions. Each recording (capture) contains multiple takes of one or more participants (camera wearer) performing a physical (Soccer, Basketball, Dance, Bouldering, and Music) or procedural (Cooking, Bike Repair, Health) task. Due to the usage of the Aria glasses, we have a wide range of associated 3D data.",source:"@site/docs/overview.md",sourceDirName:".",slug:"/overview",permalink:"/overview",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Overview",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Getting Started",permalink:"/getting-started"},next:{title:"Annotations",permalink:"/annotations/"}},s={},c=[{value:"Summary",id:"summary",level:2},{value:"Recording Devices",id:"recording-devices",level:3},{value:"Sensor Data",id:"sensor-data",level:3},{value:"Processed Data",id:"processed-data",level:3},{value:"Annotations",id:"annotations",level:3},{value:"Language",id:"language",level:3},{value:"Meta-data",id:"meta-data",level:3},{value:"Utilities",id:"utilities",level:3}],p={toc:c},u="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(u,(0,r.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"Ego-Exo4D is a large-scale multi-modal multi-view video dataset (including 3D) and benchmark challenge. The dataset consists of time-synchronized videos of participants recorded with at least one first-person (egocentric Aria glasses) and third-person (exocentric GoPro cameras) perspective cameras. Recordings occurred around the world from 12 different research institutions. Each recording (capture) contains multiple takes of one or more participants (camera wearer) performing a physical (Soccer, Basketball, Dance, Bouldering, and Music) or procedural (Cooking, Bike Repair, Health) task. Due to the usage of the Aria glasses, we have a wide range of associated 3D data."),(0,n.kt)("h2",{id:"summary"},"Summary"),(0,n.kt)("h3",{id:"recording-devices"},"Recording Devices"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"One Aria glass",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"RGB camera"),(0,n.kt)("li",{parentName:"ul"},"2 x monochrome cameras"),(0,n.kt)("li",{parentName:"ul"},"7 x microphones"),(0,n.kt)("li",{parentName:"ul"},"2 x IMU"))),(0,n.kt)("li",{parentName:"ul"},"4-5 GoPro",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"RGB camera"),(0,n.kt)("li",{parentName:"ul"},"Stereo microphone")))),(0,n.kt)("h3",{id:"sensor-data"},"Sensor Data"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Video",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"4k@60FPS for GoPro"),(0,n.kt)("li",{parentName:"ul"},"1404x1404@30FPS for Aria ","[VRS file format]"))),(0,n.kt)("li",{parentName:"ul"},"Audio",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"7 channel audio for Aria  ","[VRS file format]"),(0,n.kt)("li",{parentName:"ul"},"128kbps AAC compression, 48kHz sample rate for GoPro"))),(0,n.kt)("li",{parentName:"ul"},"IMU",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"2 x 1kHZ for Aria (left and right side)  ","[VRS file format]")))),(0,n.kt)("h3",{id:"processed-data"},"Processed Data"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Video Data",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Time-synchronized take-separated videos compressed with H264 (slow, 24, yuv420p)"),(0,n.kt)("li",{parentName:"ul"},"Collage videos of every camera available for visualization purposes"))),(0,n.kt)("li",{parentName:"ul"},"Aria\u2019s Machine Perception Services (MPS)",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Intrinsic parameters for all cameras (Aria and GoPros)"),(0,n.kt)("li",{parentName:"ul"},"Extrinsic parameters for all cameras (Aria and GoPros)"),(0,n.kt)("li",{parentName:"ul"},"Sparse 3D point clouds of static environment"))),(0,n.kt)("li",{parentName:"ul"},"3D eye gaze vectors "),(0,n.kt)("li",{parentName:"ul"},"Pre-extracted video features for all takes and associated cameras")),(0,n.kt)("h3",{id:"annotations"},"Annotations"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"../annotations/keystep"},"Keysteps"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Procedural activities time-segmented into regions classified within hierarchical taxonomy, with the intention to breakdown the high-level goal(s) into keysteps; "))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"../annotations/relations"},"Object Segmentation Mask Tracks"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Across egocentric and at least one exocentric view"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"../annotations/ego_pose"},"Human body and hand joints"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"2D Keypoints",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"17 body keypoints"),(0,n.kt)("li",{parentName:"ul"},"2 x 21 hand keypoints"))),(0,n.kt)("li",{parentName:"ul"},"3D Joint Positions",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"17 Triangulated body joints"),(0,n.kt)("li",{parentName:"ul"},"2 x 21 Triangulated hand joints")))))),(0,n.kt)("h3",{id:"language"},"Language"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"../annotations/expert_commentary"},"Expert Commentary"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Professional coaches and domain experts evaluate task performance at key moments in the videos"))),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"../annotations/atomic_descriptions"},"Atomic Actions"),(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Text descriptions for every 5 seconds of video"))),(0,n.kt)("li",{parentName:"ul"},"Narrate and Act",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Participant describes why and how as they perform their task")))),(0,n.kt)("h3",{id:"meta-data"},"Meta-data"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Task Labels",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Each take consists of a camera wearer performing a predefined task, the\ntask label is available."))),(0,n.kt)("li",{parentName:"ul"},"Participant Surveys",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"Before (pre) and after (post) survey data answered by the participant to the help asses the proficiency of the camera wearer with normalized proficiency categories per domain")))),(0,n.kt)("h3",{id:"utilities"},"Utilities"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Data downloader"),(0,n.kt)("li",{parentName:"ul"},"Data visualization website"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/Ego4d/tree/main/notebooks/egoexo"},"Example usage notebooks")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/Ego4d/tree/main?tab=readme-ov-file#summary"},"Code utilities"))),(0,n.kt)("p",null,"There are seven benchmark tasks derived from the annotations. The benchmark tasks form the Challenge we will host for 2024."))}m.isMDXComponent=!0}}]);