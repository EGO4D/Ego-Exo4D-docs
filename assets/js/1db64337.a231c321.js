"use strict";(self.webpackChunkegoexo_docs=self.webpackChunkegoexo_docs||[]).push([[372],{3905:(e,a,t)=>{t.d(a,{Zo:()=>c,kt:()=>k});var r=t(7294);function i(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function n(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function o(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?n(Object(t),!0).forEach((function(a){i(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):n(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function l(e,a){if(null==e)return{};var t,r,i=function(e,a){if(null==e)return{};var t,r,i={},n=Object.keys(e);for(r=0;r<n.length;r++)t=n[r],a.indexOf(t)>=0||(i[t]=e[t]);return i}(e,a);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);for(r=0;r<n.length;r++)t=n[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=r.createContext({}),p=function(e){var a=r.useContext(s),t=a;return e&&(t="function"==typeof e?e(a):o(o({},a),e)),t},c=function(e){var a=p(e.components);return r.createElement(s.Provider,{value:a},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},u=r.forwardRef((function(e,a){var t=e.components,i=e.mdxType,n=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(t),u=i,k=m["".concat(s,".").concat(u)]||m[u]||d[u]||n;return t?r.createElement(k,o(o({ref:a},c),{},{components:t})):r.createElement(k,o({ref:a},c))}));function k(e,a){var t=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var n=t.length,o=new Array(n);o[0]=u;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l[m]="string"==typeof e?e:i,o[1]=l;for(var p=2;p<n;p++)o[p]=t[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},6777:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>s,contentTitle:()=>o,default:()=>d,frontMatter:()=>n,metadata:()=>l,toc:()=>p});var r=t(7462),i=(t(7294),t(3905));const n={title:"Overview",sidebar_position:3},o=void 0,l={unversionedId:"overview",id:"overview",title:"Overview",description:"Ego-Exo4D is a large-scale multi-modal multi-view video dataset (including 3D) and benchmark challenge. The dataset consists of time-synchronized videos of participants recorded with at least one first-person (egocentric Aria glasses) and third-person (exocentric GoPro cameras) perspective cameras. Recordings occurred around the world from 12 different research institutions. Each recording (capture) contains multiple takes of one or more participants (camera wearer) performing a physical (Soccer, Basketball, Dance, Bouldering, and Music) or procedural (Cooking, Bike Repair, Health) task. Due to the usage of the Aria glasses, we have a wide range of associated 3D data.",source:"@site/docs/overview.md",sourceDirName:".",slug:"/overview",permalink:"/overview",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Overview",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Getting Started",permalink:"/getting-started"},next:{title:"Data",permalink:"/data/"}},s={},p=[{value:"Summary",id:"summary",level:2},{value:"Recording Devices",id:"recording-devices",level:3},{value:"Data",id:"data",level:3},{value:"Annotations",id:"annotations",level:3},{value:"Language-Video Aligned Data",id:"language-video-aligned-data",level:3},{value:"Meta-data",id:"meta-data",level:3},{value:"Utilities",id:"utilities",level:3}],c={toc:p},m="wrapper";function d(e){let{components:a,...t}=e;return(0,i.kt)(m,(0,r.Z)({},c,t,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Ego-Exo4D is a large-scale multi-modal multi-view video dataset (including 3D) and benchmark challenge. The dataset consists of time-synchronized videos of participants recorded with at least one first-person (egocentric Aria glasses) and third-person (exocentric GoPro cameras) perspective cameras. Recordings occurred around the world from 12 different research institutions. Each recording (capture) contains multiple takes of one or more participants (camera wearer) performing a physical (Soccer, Basketball, Dance, Bouldering, and Music) or procedural (Cooking, Bike Repair, Health) task. Due to the usage of the Aria glasses, we have a wide range of associated 3D data."),(0,i.kt)("h2",{id:"summary"},"Summary"),(0,i.kt)("h3",{id:"recording-devices"},"Recording Devices"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"One Aria glass",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"RGB camera"),(0,i.kt)("li",{parentName:"ul"},"2 x monochrome cameras"),(0,i.kt)("li",{parentName:"ul"},"7 x microphones"),(0,i.kt)("li",{parentName:"ul"},"2 x IMU"))),(0,i.kt)("li",{parentName:"ul"},"4-5 GoPro",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"RGB camera"),(0,i.kt)("li",{parentName:"ul"},"Stereo microphone")))),(0,i.kt)("h3",{id:"data"},"Data"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Sensor-based Data (Video, Audio, IMU)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Video, audio and IMU data ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Video: 4k@60FPS (MP4) for GoPro devices and 1404x1404@30FPS (VRS) for Aria devices"),(0,i.kt)("li",{parentName:"ul"},"Audio: 7 channel audio for Aria (VRS); 128kbps AAC compression, 48kHz, stereo audio for GoPro cameras"),(0,i.kt)("li",{parentName:"ul"},"IMU: 2 x 1kHZ for Aria (left and right side)  ","[VRS file format]"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"./data/takes"},"Take-separated & time-synchronized")," data:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"MP4 video&audio data: all camera feeds are compressed with H264 (slow, 24, yuv420p) ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Downscaled variants of the above are available (448px short-side)"))),(0,i.kt)("li",{parentName:"ul"},"Trimmed Aria VRS & trajectory data"))),(0,i.kt)("li",{parentName:"ul"},"Pre-rendered collage videos integrating all views/cameras (for visualization purposes)"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/data/mps"},"Aria\u2019s Machine Perception Services (MPS)"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Calibrated camera parameters (intrinsics) for all cameras (in VRS file)"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/data/mps#trajectory"},"3D camera poses")," (trajectories / extrinsic parameters) for all cameras"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/data/mps#point-clouds"},"Sparse 3D point clouds of static environment")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/data/mps#eye-gaze"},"3D eye gaze vectors")))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/data/features"},"Pre-extracted video features")," for all takes and associated cameras")),(0,i.kt)("h3",{id:"annotations"},"Annotations"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"../annotations/keystep"},"Keysteps"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Procedural activities time-segmented into regions classified within hierarchical taxonomy, with the intention to breakdown the high-level goal(s) into keysteps; "))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"../annotations/relations"},"Object Segmentation Mask Tracks"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Across egocentric and at least one exocentric view"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"../annotations/ego_pose"},"Human body and hand joints")," (human annotated & automatically generated)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"2D Keypoints",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"17 body keypoints"),(0,i.kt)("li",{parentName:"ul"},"2 x 21 hand keypoints"))),(0,i.kt)("li",{parentName:"ul"},"3D Joint Positions",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"17 Triangulated body joints"),(0,i.kt)("li",{parentName:"ul"},"2 x 21 Triangulated hand joints")))))),(0,i.kt)("h3",{id:"language-video-aligned-data"},"Language-Video Aligned Data"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"../annotations/expert_commentary"},"Expert Commentary")," (the ",(0,i.kt)("em",{parentName:"li"},'"what"')," from a layman's third-person perspective)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Professional coaches and domain experts evaluate task performance at key moments in the videos"))),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"../annotations/atomic_descriptions"},"Atomic Actions")," (the ",(0,i.kt)("em",{parentName:"li"},'"how"')," from an expert's third-person perspective) ",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Text descriptions at densely sampled timepoints across the video, and also includes information about the most informative view and whether the action is visible from the egocentric camera."))),(0,i.kt)("li",{parentName:"ul"},"Narrate and Act (the ",(0,i.kt)("em",{parentName:"li"},'"why and how"')," from the participant's perspective)",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Participant describes why and how as they perform their task")))),(0,i.kt)("h3",{id:"meta-data"},"Meta-data"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Task Labels",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Each take consists of a camera wearer performing a predefined task, the\ntask label is available."))),(0,i.kt)("li",{parentName:"ul"},"Participant Surveys (",(0,i.kt)("em",{parentName:"li"},"to be released"),")",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Before (pre) and after (post) survey data answered by the participant to the help asses the proficiency of the camera wearer with normalized proficiency categories per domain")))),(0,i.kt)("h3",{id:"utilities"},"Utilities"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/download"},"Data downloader")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://visualize.ego4d-data.org/"},"Data visualization website")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/Ego4d/tree/main/notebooks/egoexo"},"Example usage notebooks")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/facebookresearch/Ego4d/tree/main?tab=readme-ov-file#summary"},"Code utilities"))),(0,i.kt)("p",null,"There are seven benchmark tasks derived from the annotations. The benchmark tasks form the ",(0,i.kt)("a",{parentName:"p",href:"/challenge"},"Challenge")," we will host for 2024."))}d.isMDXComponent=!0}}]);