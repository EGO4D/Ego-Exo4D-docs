"use strict";(self.webpackChunkegoexo_docs=self.webpackChunkegoexo_docs||[]).push([[881],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>y});var r=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,i=function(e,t){if(null==e)return{};var n,r,i={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},u="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,c=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),u=l(n),m=i,y=u["".concat(c,".").concat(m)]||u[m]||d[m]||o;return n?r.createElement(y,a(a({ref:t},p),{},{components:n})):r.createElement(y,a({ref:t},p))}));function y(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,a=new Array(o);a[0]=m;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[u]="string"==typeof e?e:i,a[1]=s;for(var l=2;l<o;l++)a[l]=n[l];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},3103:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>l});var r=n(7462),i=(n(7294),n(3905));const o={title:"Fine-grained Keystep Recognition",sidebar_position:1},a=void 0,s={unversionedId:"benchmarks/keystep/keystep_recoginition",id:"benchmarks/keystep/keystep_recoginition",title:"Fine-grained Keystep Recognition",description:"Task definition",source:"@site/docs/benchmarks/keystep/keystep_recoginition.md",sourceDirName:"benchmarks/keystep",slug:"/benchmarks/keystep/keystep_recoginition",permalink:"/benchmarks/keystep/keystep_recoginition",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Fine-grained Keystep Recognition",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Keystep",permalink:"/benchmarks/keystep/"},next:{title:"Task Graph",permalink:"/benchmarks/keystep/task_graph"}},c={},l=[{value:"Task definition",id:"task-definition",level:3},{value:"Metrics",id:"metrics",level:3},{value:"Baselines",id:"baselines",level:3}],p={toc:l},u="wrapper";function d(e){let{components:t,...n}=e;return(0,i.kt)(u,(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h3",{id:"task-definition"},"Task definition"),(0,i.kt)("p",null,"This task involves recognizing fine-grained keysteps from procedural egocentric videos (at test time), using models that can leverage multiple, time-synchronized views (at training time)."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Train instance:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"- 1 ego + N exo trimmed video clips\n- Keystep label\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Test instance:")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"Input: A trimmed egocentric video clip\nOutput: Predicted keystep label\n")),(0,i.kt)("p",null,"Note that at test time, the input to the model includes just the ego-view videos (RGB only). Exo-view videos, activity and scenario names, narrations, audio and associated metadata such as eye gaze, 3D point\nclouds, camera pose, and IMU information are excluded as inputs for inference (although we encourage exploring their potential utility in training) as our ultimate goal is a vision-centric approach that performs egocentric keystep recognition."),(0,i.kt)("h3",{id:"metrics"},"Metrics"),(0,i.kt)("p",null,"We measure top-1 keystep recognition accuracy (%)"),(0,i.kt)("h3",{id:"baselines"},"Baselines"),(0,i.kt)("p",null,"Coming Soon!"))}d.isMDXComponent=!0}}]);